{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33c82bc",
   "metadata": {},
   "source": [
    "Héctor Asorey\n",
    "\n",
    "# TFM: Impacto en Redes Sociales de ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d2fbbe",
   "metadata": {},
   "source": [
    "## Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935fc0f5-901d-4f4f-9569-b47ab8411f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433ff4e5",
   "metadata": {},
   "source": [
    "## Leer datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641fa105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('TwitterChatGPT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f83a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8763e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad_na_por_columna = df.isna().sum()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(cantidad_na_por_columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e14298",
   "metadata": {},
   "source": [
    "## Cálculo métricas adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12930078",
   "metadata": {},
   "source": [
    "### Número de emoticonos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def countEmojis(text):\n",
    "    cont = 0\n",
    "    # iterate over each character in the text\n",
    "    for char in text:\n",
    "        # check if the character is an emoji\n",
    "        if char in emoji.EMOJI_DATA:\n",
    "            cont += 1\n",
    "    return cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_emojis'] = list(map(countEmojis, df['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22db9b86",
   "metadata": {},
   "source": [
    "### Número de símbolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504ae95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def contar_simbolos_de_puntuacion(texto):\n",
    "    puntuacion = string.punctuation + '¿' + '¡'\n",
    "    return sum(1 for char in texto if char in puntuacion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fde405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_punctuation'] = list(map(countEmojis, df['content']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a507044",
   "metadata": {},
   "source": [
    "### Número de tweets de cada usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b03d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_tweets'] = (df['username'].value_counts()).loc[df['username']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b244e7c",
   "metadata": {},
   "source": [
    "### Menciones de cada tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b13c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_menciones(texto):\n",
    "    mentions = re.findall(r'@\\w+', texto)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a7e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mentions'] = df['content'].apply(extraer_menciones)\n",
    "df['mentions'] = df['mentions'].apply(lambda mentions: [mention[1:] for mention in mentions])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3d454",
   "metadata": {},
   "source": [
    "### Número de menciones de cada tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b011273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_mentions'] = df['content'].apply(lambda x: x.count('@'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf6659",
   "metadata": {},
   "source": [
    "### Hashtags de cada tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_hashtags(texto):\n",
    "    hashtags = re.findall(r'#\\w+', texto)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d74d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags'] = df['content'].apply(extraer_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc38df",
   "metadata": {},
   "source": [
    "### Número de hashtags de cada tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_hashtags'] = df['hashtags'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a76e2",
   "metadata": {},
   "source": [
    "### Texto limpio de enlaces, emojis, hashtags, menciones... y eliminado de \"Stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza_texto_tweets(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r',', ' ', text)\n",
    "    text = re.sub(r'ç', 'c', text)\n",
    "    text = re.sub(r'ñ', 'n', text)\n",
    "    text = re.sub(r'\\.', '', text)\n",
    "    text = re.sub(r'á|é|í|ó|ú', lambda x: 'a' if x.group() == 'á' else 'e' if x.group() == 'é' else 'i' if x.group() == 'í' else 'o' if x.group() == 'ó' else 'u', text)\n",
    "    text = re.sub(r'[^\\x11-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'<.*>', ' ', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'\\\\x\\S+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'_', '', text)\n",
    "    text = re.sub(r'\\t', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "    text = re.sub(r'rt', '', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3380775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['content'].apply(limpieza_texto_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d859d",
   "metadata": {},
   "source": [
    "### Sentiment Analysis de cada tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b668a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96de7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentWithSpacy(x):\n",
    "    doc = nlp(x)\n",
    "    sentiment = doc._.blob.polarity\n",
    "    sentiment = round(sentiment,2)\n",
    "\n",
    "    if sentiment > 0:\n",
    "      sent_label = 1 #Positive\n",
    "    else:\n",
    "      sent_label = 0 #Negative\n",
    "    return sent_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['text'].apply(sentimentWithSpacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1fdf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('prueba1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['username', 'mentions', 'hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expanded = df2.explode('mentions').explode('hashtags').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_expanded.head(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931b6e1",
   "metadata": {},
   "source": [
    "## Grafos con Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656857cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'user': ['user1', 'user2', 'user3'],\n",
    "    'mentions': [['user2', 'user3'], ['user1'], ['user2']],\n",
    "    'hashtags': [['#python', '#data'], ['#networking'], ['#data']],\n",
    "}\n",
    "\n",
    "dataframe = pd.DataFrame(data)\n",
    "\n",
    "# Separate arrays into individual rows using explode\n",
    "df_expanded = dataframe.explode('mentions').explode('hashtags').reset_index(drop=True)\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.MultiDiGraph()\n",
    "\n",
    "# Add nodes and edges to the graph\n",
    "for _, row in df_subset.iterrows():\n",
    "    user = row['username']\n",
    "    mention = row['mentions']\n",
    "    hashtag = row['hashtags']\n",
    "\n",
    "    # Add nodes\n",
    "    G.add_node(user)\n",
    "    G.add_node(mention)\n",
    "\n",
    "    # Add edge with hashtag as an attribute\n",
    "    G.add_edge(user, mention, hashtag=hashtag)\n",
    "\n",
    "hub_score, _ = nx.hits(G)\n",
    "\n",
    "# Imprimir el Hub Score para cada nodo\n",
    "print(\"Hub Score:\")\n",
    "for node, score in hub_score.items():\n",
    "    print(f\"Nodo {node}: {score}\")\n",
    "\n",
    "nx.draw(G, with_labels = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a6100",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G, \"graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "partition = community.best_partition(G.to_undirected())\n",
    "\n",
    "# Visualizar el grafo con colores de comunidad\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(G, pos, with_la bels=True, node_color=list(partition.values()), cmap=plt.cm.rainbow)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10075f4d",
   "metadata": {},
   "source": [
    "Se va a optar por utilizar la base de datos orientada a grafos Neo4J y la herramienta de visualización Gephi para todo el tema relacionado con éstos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d3adf",
   "metadata": {},
   "source": [
    "## Grafos con Python y Neo4J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"TwitterChatGPT\"), name=\"twitterchatgpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "\n",
    "# Execute the Cypher query\n",
    "result = graph.run(\"MATCH (n) RETURN n;\")\n",
    "\n",
    "# Fetch the result\n",
    "database_name = result.evaluate()\n",
    "\n",
    "# Print or use the database name as needed\n",
    "print(database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbbf6e3",
   "metadata": {},
   "source": [
    "### Grafo1 (Usuario -- Publica --> Tweets, Usuario -- Menciona --> Usuario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a56887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'graph' is your Neo4j graph instance and 'df' is your DataFrame\n",
    "\n",
    "# Create unique User nodes\n",
    "user_set = set(df['username'].tolist() + [mention for mentions in df['mentions'].dropna() for mention in mentions])\n",
    "for user in user_set:\n",
    "    user_node = Node(\"User\", name=user)\n",
    "    query = f\"MERGE (u:User {{name: '{user}'}})\"\n",
    "    graph.run(query)\n",
    "\n",
    "# Create Tweet nodes and relationships\n",
    "for index, row in df.iterrows():\n",
    "    user_node = Node(\"User\", name=row['username'])\n",
    "    tweet_node = Node(\"Tweet\", text=row['text'])\n",
    "\n",
    "    # Create relationship for user publishing the tweet\n",
    "    query_publishes = f\"\"\"\n",
    "    MERGE (u:User {{name: '{row['username']}'}})\n",
    "    MERGE (t:Tweet {{text: '{row['text']}'}})\n",
    "    MERGE (u)-[:PUBLISHES]->(t)\n",
    "    \"\"\"\n",
    "    graph.run(query_publishes)\n",
    "\n",
    "    # Create relationships for users mentioned in the tweet\n",
    "    for mentioned_user in row['mentions']:\n",
    "        query_mentions = f\"\"\"\n",
    "        MERGE (u:User {{name: '{row['username']}'}})\n",
    "        MERGE (mu:User {{name: '{mentioned_user}'}})\n",
    "        MERGE (u)-[:MENTIONS]->(mu)\n",
    "        \"\"\"\n",
    "        graph.run(query_mentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6bc74",
   "metadata": {},
   "source": [
    "### Grafo 2 (Usuario -- Publica --> Tweets -- Menciona --> Usuarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74070827",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_completo = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe_completo.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1585371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = df\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0217d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "\n",
    "#Nodos usuarios\n",
    "for index, row in df.iterrows():\n",
    "    user_node = Node(\"User\", name=row['username'], tweet_count=row['num_tweets'])\n",
    "    query = f\"MERGE (u:User {{name: '{row['username']}', tweet_count: {row['num_tweets']}}})\"\n",
    "    graph.run(query)\n",
    "\n",
    "# Nodos tweets\n",
    "for index, row in df.iterrows():\n",
    "    #print(index)\n",
    "    tweet_node = Node(\"Tweet\", text=row['text'], date=row['date'], like_count=row['like_count'], retweet_count=row['retweet_count'],\n",
    "                      number_emojis = row['number_emojis'], number_punctuation=row['number_punctuation'], num_mentions=row['num_mentions'],\n",
    "                      hashtags=row['hashtags'], num_hashtags=row['num_hashtags'], sentiment=row['sentiment'])\n",
    "\n",
    "    # Relaciones de publicación\n",
    "    query_publishes = f\"\"\"\n",
    "    MATCH (u:User {{name: '{row['username']}', tweet_count: {row['num_tweets']}}})\n",
    "    MERGE (t:Tweet {{text: '{row['text']}', date: '{row['date']}', like_count: {row['like_count']}, retweet_count: {row['retweet_count']},\n",
    "                      number_emojis: {row['number_emojis']}, number_punctuation: {row['number_punctuation']}, num_mentions: {row['num_mentions']},\n",
    "                      hashtags: '{','.join(row['hashtags'])}', num_hashtags: {row['num_hashtags']}, sentiment: '{row['sentiment']}'}})\n",
    "    MERGE (u)-[:PUBLISHES]->(t)\n",
    "    \"\"\"\n",
    "    graph.run(query_publishes)\n",
    "\n",
    "    # Create relationships for users mentioned in the tweet\n",
    "    for mentioned_user in row['mentions']:\n",
    "        query_mentions = f\"\"\"\n",
    "        MATCH (t:Tweet {{text: '{row['text']}'}})\n",
    "        MATCH (mu:User {{name: '{mentioned_user}'}})\n",
    "        MERGE (t)-[:MENTIONS]->(mu)\n",
    "        \"\"\"\n",
    "        #print(query_mentions)\n",
    "        graph.run(query_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dca270",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (u1:User)-[:PUBLISHES]->(t:Tweet)-[:MENTIONS]->(u2:User)\n",
    "MERGE (u1)-[m:MENTIONS_DIRECTLY]->(u2)\n",
    "SET m.text = t.text, m.hashtags = t.hashtags\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30537688",
   "metadata": {},
   "source": [
    "### Añadir a Neo4J el sentimiento general de los usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599359bd",
   "metadata": {},
   "source": [
    "Leer el dataframe con todo (embeddings y sentimientos) y hacer las transformaciones necesarias para que sea valido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasetConVaderV2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e5237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df[\"hashtags\"] = df['hashtags'].apply(ast.literal_eval)\n",
    "df[\"mentions\"] = df['mentions'].apply(ast.literal_eval)\n",
    "df = df.fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0befb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d659a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61e80e0",
   "metadata": {},
   "source": [
    "Función general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_sentiment = df.groupby('username')['sentiment'].mean().round().astype(int)\n",
    "\n",
    "# Merge the overall sentiment back to the original DataFrame\n",
    "df = pd.merge(df, overall_sentiment, how='left', on='username', suffixes=('', '_overall'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_round(x):\n",
    "    if x >= 0.33:\n",
    "        return 1\n",
    "    elif x <= -0.33:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "# Ejemplo de uso:\n",
    "resultado = custom_round(-0.33)\n",
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834498ff",
   "metadata": {},
   "source": [
    "Función para calcular el overall de los sentimientos con Vader si queremos que haya sentimiento \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec89fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_sentiment = df.groupby('username')['sentimentVaderWithNeutral'].mean().round().astype(int)\n",
    "\n",
    "# Merge the overall sentiment back to the original DataFrame\n",
    "df = pd.merge(df, overall_sentiment, how='left', on='username', suffixes=('', '_overallV2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba8528",
   "metadata": {},
   "source": [
    "Función para calcular el overall de los sentimientos con Vader personalizado con sentimiento neutral\n",
    "\n",
    "Con esta función se fuerza a que el neutro sea realmente neutro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_sentiment = df.groupby('username')['sentimentVaderWithNeutral'].mean().apply(custom_round).astype(int)\n",
    "#df = df.drop(columns=['sentimentVaderWithNeutral_overallV2'], axis=1)\n",
    "df = pd.merge(df, overall_sentiment, how='left', on='username', suffixes=('', '_overallV2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.head(100000) #50000 Probar con 100000 \n",
    "#En la base de datos de Neo4J hay 100000 usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3409ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.tail(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7780a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentioned_users = data.apply(lambda row: set(row['mentions']).intersection(set(data['username'])), axis=1)\n",
    "mentioned_users = mentioned_users[mentioned_users.apply(lambda x: len(x) > 0)].apply(lambda x: list(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86f2bb",
   "metadata": {},
   "source": [
    "Función general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear un nuevo DataFrame para almacenar los resultados\n",
    "new_rows = []\n",
    "\n",
    "# Iterar sobre los usuarios mencionados que no han publicado tweets\n",
    "for user in mentioned_users:\n",
    "    # Filtrar tweets donde el usuario ha sido mencionado\n",
    "    user_mentions = data[data['mentions'].apply(lambda x: user in x)]\n",
    "\n",
    "    # Calcular el sentiment promedio para el usuario basado en los tweets en los que ha sido mencionado\n",
    "    overall_sentiment = user_mentions['sentiment'].mean().round()\n",
    "\n",
    "    # Crear una nueva fila con la información calculada\n",
    "    new_row = {'username': user, 'overall_sentiment': overall_sentiment}\n",
    "\n",
    "    # Agregar la nueva fila al DataFrame de resultados\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "# Crear un nuevo DataFrame con los resultados\n",
    "result_df = pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799fbeb",
   "metadata": {},
   "source": [
    "Función especializada para el caso de Vader con sentimiento \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.05\n",
    "# Crear un nuevo DataFrame para almacenar los resultados\n",
    "new_rows = []\n",
    "\n",
    "# Iterar sobre los usuarios mencionados que no han publicado tweets\n",
    "for user in mentioned_users:\n",
    "    # Filtrar tweets donde el usuario ha sido mencionado\n",
    "    user_mentions = data[data['mentions'].apply(lambda x: user in x)]\n",
    "\n",
    "    if user_mentions['sentimentVaderWithNeutral'].mean() < -0.33:\n",
    "        overall_sentiment = -1\n",
    "    elif user_mentions['sentimentVaderWithNeutral'].mean() > 0.33:\n",
    "        overall_sentiment = 1\n",
    "    else:\n",
    "        overall_sentiment = 0\n",
    "        \n",
    "    # Crear una nueva fila con la información calculada\n",
    "    new_row = {'username': user, 'overall_sentiment': overall_sentiment}\n",
    "\n",
    "    # Agregar la nueva fila al DataFrame de resultados\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "# Crear un nuevo DataFrame con los resultados\n",
    "result_df = pd.DataFrame(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['overall_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentimentVaderWithNeutral_overallV2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8240fb",
   "metadata": {},
   "source": [
    "Añadir sentimiento a los usuarios que publican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822fc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "\n",
    "# Assuming 'graph' is your Neo4j graph instance and 'df' is your DataFrame\n",
    "# Assuming you have calculated overall_sentiment as described before\n",
    "\n",
    "# Update User nodes with overall sentiment\n",
    "for index, row in data.reset_index().iterrows():\n",
    "    query_update_user = f\"\"\"\n",
    "    MATCH (u:User {{name: '{row['username']}'}})\n",
    "    SET u.overall_sentiment = {row['sentiment_overall']}\n",
    "    \"\"\"\n",
    "    graph.run(query_update_user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a42f86",
   "metadata": {},
   "source": [
    "Añadir sentimiento Vader con \"neutral\" a los usuarios que publican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01004c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "\n",
    "# Assuming 'graph' is your Neo4j graph instance and 'df' is your DataFrame\n",
    "# Assuming you have calculated overall_sentiment as described before\n",
    "\n",
    "# Update User nodes with overall sentiment\n",
    "for index, row in data.reset_index().iterrows():\n",
    "    query_update_user = f\"\"\"\n",
    "    MATCH (u:User {{name: '{row['username']}'}})\n",
    "    SET u.overall_sentiment_vader_with_neutral = {row['sentimentVaderWithNeutral_overall']}\n",
    "    \"\"\"\n",
    "    graph.run(query_update_user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8014eea8",
   "metadata": {},
   "source": [
    "Añadir sentimiento a los usuarios mencionados que no publican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "\n",
    "# Assuming 'graph' is your Neo4j graph instance and 'df' is your DataFrame\n",
    "# Assuming you have calculated overall_sentiment as described before\n",
    "\n",
    "# Update User nodes with overall sentiment\n",
    "for index, row in result_df.reset_index().iterrows():\n",
    "    query_update_user = f\"\"\"\n",
    "    MATCH (u:User {{name: '{row['username']}'}})\n",
    "    SET u.overall_sentiment = {row['overall_sentiment']}\n",
    "    \"\"\"\n",
    "    graph.run(query_update_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab13674f",
   "metadata": {},
   "source": [
    "Añadir sentimiento vader con \"neutral\" a los usuarios mencionados que no publican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435eddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node\n",
    "\n",
    "# Assuming 'graph' is your Neo4j graph instance and 'df' is your DataFrame\n",
    "# Assuming you have calculated overall_sentiment as described before\n",
    "\n",
    "# Update User nodes with overall sentiment\n",
    "for index, row in result_df.reset_index().iterrows():\n",
    "    query_update_user = f\"\"\"\n",
    "    MATCH (u:User {{name: '{row['username']}'}})\n",
    "    SET u.overall_sentiment_vader_with_neutral = {row['overall_sentiment']}\n",
    "    \"\"\"\n",
    "    graph.run(query_update_user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349ff4d",
   "metadata": {},
   "source": [
    "## Spacy en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ae25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "def sentimentWithSpacyLarge(x):\n",
    "    doc = nlp(x)\n",
    "    sentiment = doc._.blob.polarity\n",
    "    sentiment = round(sentiment,2)\n",
    "\n",
    "    if sentiment > 0:\n",
    "      sent_label = 1 #Positive\n",
    "    else:\n",
    "      sent_label = 0 #Negative\n",
    "    return sent_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentimentLg'] = data['text'].apply(sentimentWithSpacyLarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae01bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['different_sentiments'] = data['sentiment'] != data['sentimentLg']\n",
    "\n",
    "different_sentiments_count = data['different_sentiments'].sum()\n",
    "\n",
    "print(f\"Number of rows with different sentiments: {different_sentiments_count}\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090d4f26",
   "metadata": {},
   "source": [
    "Como parecen devolver los mismos valores, no actualizaremos la base de datos de Neo4J con los resultados del modelo \"en_core_web_lg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430267c1",
   "metadata": {},
   "source": [
    "## VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a35d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b3510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores_with_neutral(sentence):\n",
    " \n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    " \n",
    "    if sentiment_dict['compound'] >= 0.05 :\n",
    "        return 1\n",
    " \n",
    "    elif sentiment_dict['compound'] <= - 0.05 :\n",
    "        return -1\n",
    " \n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90828063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores_without_neutral(sentence):\n",
    " \n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    " \n",
    "    if sentiment_dict['compound'] >= 0 :\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ca548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentimentVaderWithNeutral\"] = df['text'].apply(sentiment_scores_with_neutral)\n",
    "df[\"sentimentVaderWithoutNeutral\"] = df['text'].apply(sentiment_scores_without_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfaf81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('datasetConVader.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088a7f4",
   "metadata": {},
   "source": [
    "## Creación de embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be1cb08",
   "metadata": {},
   "source": [
    "Lo primero es leer el dataset creado anteriormente si no está cargado en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('prueba1.csv')\n",
    "\n",
    "import ast\n",
    "\n",
    "df[\"hashtags\"] = df['hashtags'].apply(ast.literal_eval)\n",
    "df[\"mentions\"] = df['mentions'].apply(ast.literal_eval)\n",
    "df = df.fillna('')\n",
    "\n",
    "df = df.head(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01f7e8",
   "metadata": {},
   "source": [
    "Descarga del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203baf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df['text'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('datasetConEmbeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ce010",
   "metadata": {},
   "source": [
    "## Exploración de los Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0605e",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "Debido al coste computacional, al igual que la parte de Neo4J, solo se ejecutará con los 100000 primeros registros..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2793d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df = pd.read_csv('datasetConVader.csv')\n",
    "df[\"hashtags\"] = df['hashtags'].apply(ast.literal_eval)\n",
    "df[\"mentions\"] = df['mentions'].apply(ast.literal_eval)\n",
    "df = df.fillna('')\n",
    "df['embeddings'] = list(map(lambda x: [float(num) for num in x.strip('[]').split()], df['embeddings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa20966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.head(100000)\n",
    "del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabaceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(data['embeddings'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79fcd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = NearestNeighbors(n_neighbors=25)\n",
    "neighbors_fit = neighbors.fit(normalized_data)\n",
    "distances, indices = neighbors_fit.kneighbors(normalized_data)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2361b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 20  \n",
    "min_samples = 25 #Valores a utilizar: 50 \n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DBSCAN_labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_label = 10\n",
    "\n",
    "# Iterate over each unique value in the DBSCAN column\n",
    "for label in data['DBSCAN_labels'].unique():\n",
    "    # Filter DataFrame to include only samples with the current DBSCAN label\n",
    "    filtered_df = data[data['DBSCAN_labels'] == label]\n",
    "    \n",
    "    # Display 20 text samples from the current DBSCAN label\n",
    "    print(f\"DBSCAN Label: {label}\")\n",
    "    print(filtered_df['content'].head(samples_per_label).values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366107c7",
   "metadata": {},
   "source": [
    "Eliminamos aquellos usuarios que parecen bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d907fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_by_labels = data[~data['DBSCAN_labels'].isin([1, 2, 4, 7])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc371b0",
   "metadata": {},
   "source": [
    "Volvemos a aplicar DBSCAN, habiendo quitado las comunidades de bots, pero esta vez con más características aparte de solo el contenido de tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0606c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_by_labels_v2 = filtered_df_by_labels.drop(['DBSCAN_labels', 'content', 'date', 'id', 'username', 'mentions', 'hashtags', 'text'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93081d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_by_labels_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302a5fb",
   "metadata": {},
   "source": [
    "Separamos los embeddings en distintas columnas en vez de estar almacenados en una lista (esto es para poder aplicar los modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfe5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = filtered_df_by_labels['embeddings'].apply(pd.Series)\n",
    "new_df.columns = ['embeddings{}'.format(x) for x in new_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_to_analyze = pd.concat([filtered_df_by_labels_v2, new_df], axis=1)\n",
    "del(new_df)\n",
    "dataframe_to_analyze = dataframe_to_analyze.drop(['embeddings'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde79f65",
   "metadata": {},
   "source": [
    "Normalizar el dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d475509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 3000))\n",
    "\n",
    "# normalize the DataFrame\n",
    "dataframe_to_analyze_norm = pd.DataFrame(scaler.fit_transform(dataframe_to_analyze), columns=dataframe_to_analyze.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6059140",
   "metadata": {},
   "source": [
    "Si queremos utilizar PCA para hacer más sencilla la computación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embedding_features = dataframe_to_analyze_norm.iloc[:, 10:]\n",
    "\n",
    "pca = PCA(n_components=70) \n",
    "embedding_features_reduced = pca.fit_transform(embedding_features)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_variance = 0\n",
    "for i, ratio in enumerate(explained_variance_ratio):\n",
    "    cumulative_variance += ratio\n",
    "    print(f\"Variance explained by PC{i+1}: {ratio:.2%} (Cumulative: {cumulative_variance:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del(dataframe_to_analyze)\n",
    "del(filtered_df_by_labels_v2)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4297c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embedding_df = pd.DataFrame(embedding_features_reduced, columns=[f'PCA_{i}' for i in range(1, 71)])\n",
    "\n",
    "dataframe_to_analyze_norm_reduced = dataframe_to_analyze_norm.drop(dataframe_to_analyze_norm.columns[10:], axis=1)\n",
    "dataframe_reduced = pd.concat([dataframe_to_analyze_norm_reduced, pca_embedding_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToAnalyze = dataframe_reduced.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc811218",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframe_reduced\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eced0f9",
   "metadata": {},
   "source": [
    "Si queremos utilizar todos los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataToAnalyze = dataframe_to_analyze_norm.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a3095",
   "metadata": {},
   "source": [
    "Cambiamos de dataframe a lista de números (esto es porque con DataFrame falta memoria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f76f5a",
   "metadata": {},
   "source": [
    "Aplicar DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = NearestNeighbors(n_neighbors=20)\n",
    "neighbors_fit = neighbors.fit(dataToAnalyze)\n",
    "distances, indices = neighbors_fit.kneighbors(dataToAnalyze)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb0eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 6000  \n",
    "min_samples = 50 #Valores a utilizar: 50 \n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(dataToAnalyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334227d",
   "metadata": {},
   "source": [
    "En caso de que ya existan las etiquetas (porque se quieran ver las diferencias del DBSCAN entre usar PCA o no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutar la línea de debajo solo en caso de que se haya guardado una columna llamada DBSCAN_labels_after\n",
    "#filtered_df_by_labels = filtered_df_by_labels.drop(['DBSCAN_labels_after'], axis = 1)\n",
    "filtered_df_by_labels.loc[:, 'DBSCAN_labels_after'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3073de",
   "metadata": {},
   "source": [
    "Despues de eliminar los bots, miramos los grupos formados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_label = 20\n",
    "\n",
    "# Iterate over each unique value in the DBSCAN column\n",
    "for label in filtered_df_by_labels['DBSCAN_labels_after'].unique():\n",
    "    # Filter DataFrame to include only samples with the current DBSCAN label\n",
    "    filtered_df = filtered_df_by_labels[filtered_df_by_labels['DBSCAN_labels_after'] == label]\n",
    "    \n",
    "    # Display 20 text samples from the current DBSCAN label\n",
    "    print(f\"DBSCAN Label: {label}\")\n",
    "    print(filtered_df['content'].head(samples_per_label).values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf1a94",
   "metadata": {},
   "source": [
    "Sigue habiendo bots, por tanto, los volvemos a eliminar\n",
    "\n",
    "Debemos fijarnos en los grupos que son de bots para ejecutar la línea siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae18ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_by_labels_v3 = filtered_df_by_labels[~filtered_df_by_labels['DBSCAN_labels_after'].isin([1, 2, 3, 4, 5, 6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1cdb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "del filtered_df_by_labels\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026dfb1",
   "metadata": {},
   "source": [
    "Volvemos a aplicar todos los pasos anteriores para aplicar DBSCAN, a ver si se han eliminado definitivamente los bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf325bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_by_labels_v4 = filtered_df_by_labels_v3.drop(['DBSCAN_labels', 'content', 'date', 'id', 'username', 'mentions', 'hashtags', 'text'], axis = 1)\n",
    "\n",
    "new_df = filtered_df_by_labels_v3['embeddings'].apply(pd.Series)\n",
    "new_df.columns = ['embeddings{}'.format(x) for x in new_df.columns]\n",
    "\n",
    "dataframe_to_analyze = pd.concat([filtered_df_by_labels_v4, new_df], axis=1)\n",
    "del(new_df)\n",
    "dataframe_to_analyze = dataframe_to_analyze.drop(['embeddings'], axis=1)\n",
    "\n",
    "dataframe_to_analyze_norm = pd.DataFrame(scaler.fit_transform(dataframe_to_analyze), columns=dataframe_to_analyze.columns)\n",
    "\n",
    "dataToAnalyze = dataframe_to_analyze_norm.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8957058",
   "metadata": {},
   "source": [
    "Si se quiere utilizar PCA, ejecutar la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68683ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "embedding_features = dataframe_to_analyze_norm.iloc[:, 10:]\n",
    "\n",
    "pca = PCA(n_components=70) \n",
    "embedding_features_reduced = pca.fit_transform(embedding_features)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_variance = 0\n",
    "for i, ratio in enumerate(explained_variance_ratio):\n",
    "    cumulative_variance += ratio\n",
    "    print(f\"Variance explained by PC{i+1}: {ratio:.2%} (Cumulative: {cumulative_variance:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ff362",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = NearestNeighbors(n_neighbors=20)\n",
    "neighbors_fit = neighbors.fit(dataToAnalyze)\n",
    "distances, indices = neighbors_fit.kneighbors(dataToAnalyze)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c829df",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 6000  \n",
    "min_samples = 50 #Valores a utilizar: 50 \n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = dbscan.fit_predict(dataToAnalyze)\n",
    "\n",
    "#Ejecutar la línea de debajo solo en caso de que se haya guardado una columna llamada DBSCAN_labels_after\n",
    "#filtered_df_by_labels_v3 = filtered_df_by_labels_v3.drop(['DBSCAN_labels_after_iter2'], axis = 1)\n",
    "filtered_df_by_labels_v3.loc[:, 'DBSCAN_labels_after_iter2'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ee188",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_label = 20\n",
    "\n",
    "# Iterate over each unique value in the DBSCAN column\n",
    "for label in filtered_df_by_labels_v3['DBSCAN_labels_after_iter2'].unique():\n",
    "    # Filter DataFrame to include only samples with the current DBSCAN label\n",
    "    filtered_df = filtered_df_by_labels_v3[filtered_df_by_labels_v3['DBSCAN_labels_after_iter2'] == label]\n",
    "    \n",
    "    # Display 20 text samples from the current DBSCAN label\n",
    "    print(f\"DBSCAN Label: {label}\")\n",
    "    print(filtered_df['content'].head(samples_per_label).values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6032f42",
   "metadata": {},
   "source": [
    "Prueba con OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "optics_model = OPTICS(min_samples=20, xi=0.05, min_cluster_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "optics_model.fit(embedding_features_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f947b099",
   "metadata": {},
   "source": [
    "Devuelve un único cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc94a6",
   "metadata": {},
   "source": [
    "### Visualización con t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ee6edd",
   "metadata": {},
   "source": [
    "Aplicamos TSNE para una visualización de los contenidos publicados tras haber eliminado a los bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = filtered_df_by_labels_v3['DBSCAN_labels_after_iter2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf83afd7",
   "metadata": {},
   "source": [
    "Gráfico en dos dimensiones..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf6841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=33)\n",
    "X_tsne = tsne.fit_transform(dataToAnalyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd7cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=33)\n",
    "X_tsne = tsne.fit_transform(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b38973",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = labels.astype(np.float)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, cmap='viridis', s = 10, alpha = 0.6)\n",
    "plt.colorbar()\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "plt.figure(figsize=(18,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b083e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = labels.astype(np.float)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, cmap='viridis', s = 10, alpha = 0.6)\n",
    "plt.colorbar()\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "plt.figure(figsize=(18,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c38f8",
   "metadata": {},
   "source": [
    "Gráfico en tres dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ce4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=33)\n",
    "X_tsne = tsne.fit_transform(dataToAnalyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894ecf4",
   "metadata": {},
   "source": [
    "Gráfico con matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_tsne contains your 3D data points and labels contains the color information\n",
    "\n",
    "# Convert labels to float for color mapping\n",
    "colors = labels.astype(np.float)\n",
    "\n",
    "# Create a new figure\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], c=colors, cmap='viridis', s=10, alpha=0.6)\n",
    "\n",
    "# Colorbar\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e40b9a",
   "metadata": {},
   "source": [
    "Gráfico con plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbab897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Assuming X_tsne contains your 3D data points and labels contains the color information\n",
    "\n",
    "# Create the scatter plot trace\n",
    "trace = go.Scatter3d(\n",
    "    x=X_tsne[:, 0],\n",
    "    y=X_tsne[:, 1],\n",
    "    z=X_tsne[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,  # Adjust the size as needed\n",
    "        color=labels,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.6\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=0),\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Show the interactive plot\n",
    "pio.show(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6538d",
   "metadata": {},
   "source": [
    "## Exploración de los usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c60da",
   "metadata": {},
   "source": [
    "El objetivo es agrupar a los usuarios en comunidades basándonos en sus características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822c8c6",
   "metadata": {},
   "source": [
    "### Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484af8bd",
   "metadata": {},
   "source": [
    "Se debe crear primero un dataframe de usuarios a partir del dataframe procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f8d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df_temp = pd.read_csv('datasetConVaderV2.csv')\n",
    "\n",
    "df_temp[\"hashtags\"] = df_temp['hashtags'].apply(ast.literal_eval)\n",
    "df_temp[\"mentions\"] = df_temp['mentions'].apply(ast.literal_eval)\n",
    "df_temp = df_temp.fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebbbd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08109e4f",
   "metadata": {},
   "source": [
    "Creamos un dataframe de los usuarios con sus estadísticas numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUsers = df_temp.groupby('username').agg(\n",
    "                                          num_tweets=('num_tweets', 'first'),\n",
    "                                          avg_likes=('like_count', 'mean'),\n",
    "                                          avg_retweets=('retweet_count', 'mean'),\n",
    "                                          avg_hashtags=('num_hashtags', 'mean'),\n",
    "                                          avg_mentions=('num_mentions', 'mean'),\n",
    "                                          avg_emojis=('number_emojis', 'mean'),\n",
    "                                          avg_punctuation=('number_punctuation', 'mean'),\n",
    "                                          avg_sentiment_vader=('sentimentVaderWithoutNeutral', 'mean'),\n",
    "                                          avg_sentiment_vader_neutral=('sentimentVaderWithNeutral', 'mean'),\n",
    "                                          avg_sentiment=('sentiment', 'mean')).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e123c4",
   "metadata": {},
   "source": [
    "Vemos cuantas veces ha sido mencionado cada usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408aaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df_temp[['username', 'mentions']]\n",
    "\n",
    "flattened_users = df_temp['mentions'].explode()\n",
    "\n",
    "flattened_users\n",
    "user_mentions_count = flattened_users.value_counts().reset_index()\n",
    "user_mentions_count.columns = ['user', 'mention_count']\n",
    "user_mentions_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5a309",
   "metadata": {},
   "source": [
    "Añadimos esta información al dataframe con estadísticas de los usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUsers = dfUsers.merge(user_mentions_count, left_on='username', right_on='user', how='left')\n",
    "dfUsers.drop(columns=['user'], inplace=True)\n",
    "dfUsers['mention_count'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee120938",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUsers.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca5b50",
   "metadata": {},
   "source": [
    "### Crear comunidades de usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef7331",
   "metadata": {},
   "source": [
    "Aplicar OPTICS con estos datos\n",
    "\n",
    "(no se han logrado resultados con DBSCAN debido a que hay demasiados usuarios, es decir, por coste computacional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOPTICS = dfUsers.drop_duplicates().drop(['username'], axis = 1).head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608efa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "dfOPTICSNorm = pd.DataFrame(scaler.fit_transform(dfOPTICS), columns=dfOPTICS.columns)\n",
    "dataToAnalyze = dfOPTICSNorm.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea6858",
   "metadata": {},
   "source": [
    "Modelo OPTICS con parámetros por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74063e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "#Min samples controla cuantos puntos tiene que tener alrededor otro punto para ser considerado central\n",
    "#Xi controla los clusters formados, de tal manera que xi pequeño facilita la creación de clusters pequeños (aumenta la sensitividad a variaciones de densidad)\n",
    "#Min cluster size representa que porcentaje de los datos frente al total de datos en el dataset son necesarios para formar un cluster\n",
    "optics_model = OPTICS(min_samples=20, xi=0.05, min_cluster_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optics_model.fit(dataToAnalyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcde9cfe",
   "metadata": {},
   "source": [
    "<p> 50000 datos ----> 1 min 34 secs </p>\n",
    "<p> 100000 datos ----> 6 min 54 secs </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ef2f0",
   "metadata": {},
   "source": [
    "Modelo OPTICS con parámetros propios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967374b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optics_model = OPTICS(min_samples=20, xi=0.05, min_cluster_size=0.025)\n",
    "optics_model.fit(dataToAnalyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa61e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(np.unique(optics_model.labels_, return_counts=True)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57237f40",
   "metadata": {},
   "source": [
    "### Visualizar las comunidades de usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0313e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=33)\n",
    "X_tsne = tsne.fit_transform(dataToAnalyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90653bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = optics_model.labels_.astype(np.float64)\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, cmap='viridis', s = 10, alpha = 0.6)\n",
    "plt.colorbar()\n",
    "plt.rcParams[\"figure.figsize\"] = (18, 15)\n",
    "plt.figure(figsize=(18,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad693d",
   "metadata": {},
   "source": [
    "Gráfico interactivo en 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b207bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, random_state=33)\n",
    "X_tsne = tsne.fit_transform(dataToAnalyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fde1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Assuming X_tsne contains your 3D data points and labels contains the color information\n",
    "\n",
    "# Create the scatter plot trace\n",
    "trace = go.Scatter3d(\n",
    "    x=X_tsne[:, 0],\n",
    "    y=X_tsne[:, 1],\n",
    "    z=X_tsne[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,  # Adjust the size as needed\n",
    "        color=optics_model.labels_.astype(np.float64),\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.6\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    margin=dict(l=0, r=0, b=0, t=0),\n",
    "    width=1000,  # Adjust the width as needed\n",
    "    height=800,\n",
    ")\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "# Show the interactive plot\n",
    "pio.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d6be61",
   "metadata": {},
   "source": [
    "### Análisis de las comunidades creadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUsersToAnalyze = dfUsers.drop_duplicates().head(100000)\n",
    "dfUsersToAnalyze['labelsOPTICS'] = optics_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4266c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = dfUsersToAnalyze.drop(['username'], axis=1).groupby('labelsOPTICS').mean()\n",
    "\n",
    "# Print the averages\n",
    "print(\"Averages of each metric for each label:\")\n",
    "print(averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.colormaps['tab10']\n",
    "colors = [cmap(i/len(averages.index)) for i in range(len(averages.index))]\n",
    "\n",
    "for col in averages.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, (label, values) in enumerate(averages.iterrows()):\n",
    "        if values[col] == 0.0:\n",
    "            plt.plot(label, values[col], marker='_', color=colors[i], markersize=40, linewidth=400)\n",
    "        else:\n",
    "            plt.bar(label, values[col], color=colors[i], label=label)\n",
    "    plt.title(f'Average {col} for Each Label')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Average Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f6746",
   "metadata": {},
   "source": [
    "### Análisis de ciertos usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0615b65",
   "metadata": {},
   "source": [
    "Se van a analizar algunos de los usuarios que hay en la base de datos de Neo4J, según la métrica HITS\n",
    "\n",
    "Así, se van a mirar las métricas de los usuarios con mayor authority score y los usuarios con mayor hub score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32535da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAuthority = pd.read_csv('mostAuthorityUsersWithVader.csv')\n",
    "dfHub = pd.read_csv('mostHubUsersWithVader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAuthorityAnalysis = dfUsersToAnalyze[dfUsersToAnalyze['username'].isin(dfAuthority['name'])]\n",
    "means = dfAuthorityAnalysis.drop(['labelsOPTICS', 'username'], axis = 1).mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHubAnalysis = dfUsersToAnalyze[dfUsersToAnalyze['username'].isin(dfHub['name'])]\n",
    "means = dfHubAnalysis.drop(['labelsOPTICS', 'username'], axis = 1).mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fd892d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b623f2a",
   "metadata": {},
   "source": [
    "## Analisis de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporal = pd.read_csv('mostRelevantUsersWithVader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c14067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temporal['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfSelectedUsers = df[df['username'].isin(df_temporal['name'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722adf65",
   "metadata": {},
   "source": [
    "(En caso de querer utilizar todo el dataframe, no solo el específico para algunos usuarios, utilizar este otro bloque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfSelectedUsers = pd.read_csv('datasetConVaderV2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f12cb",
   "metadata": {},
   "source": [
    "### Nubes de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5165ec",
   "metadata": {},
   "source": [
    "Nube de palabras del contenido de todos los tweets de estos usuarios (De los 100000 primeros datos, los que más conexiones tienen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c3f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsers['text'].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7ec61",
   "metadata": {},
   "source": [
    "Transformaciones necesarias de la columna hashtags (debido a la lectura de csv, el array se transforma en string, y se debe revertir esto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "dfSelectedUsers['hashtags'] = dfSelectedUsers['hashtags'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c70f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags = [hashtag for sublist in dfSelectedUsers['hashtags'] for hashtag in sublist]\n",
    "hashtags_string = ' '.join(all_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447f9064",
   "metadata": {},
   "source": [
    "Nube de palabras de los hashtags de todos los tweets de estos usuarios (De los 100000 primeros datos, los que más conexiones tienen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(hashtags_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9b325",
   "metadata": {},
   "source": [
    "Ahora se va a hacer lo mismo, pero filtrando además por el sentimiento del tweet, para ver que diferencia de contenido hay entre tweets positivos y negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feec4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSelectedUsersPositive = dfSelectedUsers[dfSelectedUsers['sentiment'] == 1]\n",
    "dfSelectedUsersNegative = dfSelectedUsers[dfSelectedUsers['sentiment'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f4066",
   "metadata": {},
   "source": [
    "Lo mismo, pero en el caso de utilizar Vader con sentimiento neutro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ed225",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSelectedUsersPositive = dfSelectedUsers[dfSelectedUsers['sentimentVaderWithNeutral_overall'] == 1]\n",
    "dfSelectedUsersNeutral = dfSelectedUsers[dfSelectedUsers['sentimentVaderWithNeutral_overall'] == 0]\n",
    "dfSelectedUsersNegative = dfSelectedUsers[dfSelectedUsers['sentimentVaderWithNeutral_overall'] == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae05b29",
   "metadata": {},
   "source": [
    "(En caso de querer utilizar todo el dataframe, no solo el específico para algunos usuarios, utilizar este otro bloque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a726158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSelectedUsersPositive = dfSelectedUsers[dfSelectedUsers['sentimentVaderWithNeutral'] == 1]\n",
    "dfSelectedUsersNeutral = dfSelectedUsers[dfSelectedUsers['sentimentVaderWithNeutral'] == 0]\n",
    "dfSelectedUsersNegative = dfSelectedUsers[dfSelectedUsers['sentimentVaderWithNeutral'] == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a4666",
   "metadata": {},
   "source": [
    "Representar esas nubes de palabras en caso de solo sentimientos positivos y negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloudPositiveTweets = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersPositive['text'].astype(str)))\n",
    "wordcloudNegativeTweets = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersNegative['text'].astype(str)))\n",
    "\n",
    "wordcloudPositiveHashtags = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersPositive['hashtags'].astype(str)))\n",
    "wordcloudNegativeHashtags = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersNegative['hashtags'].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d070c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "\n",
    "wordclouds = [\n",
    "    (wordcloudPositiveTweets, 'Positive Tweets'),\n",
    "    (wordcloudNegativeTweets, 'Negative Tweets'),\n",
    "    (wordcloudPositiveHashtags, 'Positive Hashtags'),\n",
    "    (wordcloudNegativeHashtags, 'Negative Hashtags')\n",
    "]\n",
    "\n",
    "for (ax, (wordcloud, title)) in zip(axes.flat, wordclouds):\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb946df9",
   "metadata": {},
   "source": [
    "Lo mismo, pero un gráfico tras otro (esto se utiliza para tener las imágenes en grande para el TFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the word clouds and their titles\n",
    "wordclouds = [\n",
    "    (wordcloudPositiveTweets, 'Positive Tweets'),\n",
    "    (wordcloudNegativeTweets, 'Negative Tweets'),\n",
    "    (wordcloudPositiveHashtags, 'Positive Hashtags'),\n",
    "    (wordcloudNegativeHashtags, 'Negative Hashtags')\n",
    "]\n",
    "\n",
    "# Iterate over each word cloud and title\n",
    "for wordcloud, title in wordclouds:\n",
    "    # Create a new figure for each word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show the current word cloud\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e8abb",
   "metadata": {},
   "source": [
    "Lo mismo, pero en caso de utilizar sentimientos positivos, negativos y neutros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc587f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloudPositiveTweets = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersPositive['text'].astype(str)))\n",
    "wordcloudNegativeTweets = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersNegative['text'].astype(str)))\n",
    "wordcloudNeutralTweets = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersNeutral['text'].astype(str)))\n",
    "\n",
    "\n",
    "wordcloudPositiveHashtags = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersPositive['hashtags'].astype(str)))\n",
    "wordcloudNegativeHashtags = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersNegative['hashtags'].astype(str)))\n",
    "wordcloudNeutralHashtags = WordCloud(width=800, height=400, background_color='white').generate(' '.join(dfSelectedUsersNeutral['hashtags'].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure and axis for subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 10))\n",
    "\n",
    "# Define the word clouds and their titles\n",
    "wordclouds = [\n",
    "    (wordcloudPositiveTweets, 'Positive Tweets'),\n",
    "    (wordcloudNeutralTweets, 'Neutral Tweets'),\n",
    "    (wordcloudNegativeTweets, 'Negative Tweets'),\n",
    "    (wordcloudPositiveHashtags, 'Positive Hashtags'),\n",
    "    (wordcloudNeutralHashtags, 'Neutral Hashtags'),\n",
    "    (wordcloudNegativeHashtags, 'Negative Hashtags')\n",
    "]\n",
    "\n",
    "# Iterate over each subplot and corresponding word cloud with title\n",
    "for (ax, (wordcloud, title)) in zip(axes.flat, wordclouds):\n",
    "    # Plot the word cloud on the current subplot\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05162d6",
   "metadata": {},
   "source": [
    "Lo mismo, pero un gráfico tras otro (esto se utiliza para tener las imágenes en grande para el TFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordclouds = [\n",
    "    (wordcloudPositiveTweets, 'Positive Tweets'),\n",
    "    (wordcloudNeutralTweets, 'Neutral Tweets'),\n",
    "    (wordcloudNegativeTweets, 'Negative Tweets'),\n",
    "    (wordcloudPositiveHashtags, 'Positive Hashtags'),\n",
    "    (wordcloudNeutralHashtags, 'Neutral Hashtags'),\n",
    "    (wordcloudNegativeHashtags, 'Negative Hashtags')\n",
    "]\n",
    "\n",
    "\n",
    "for wordcloud, title in wordclouds:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61971361",
   "metadata": {},
   "source": [
    "### Sentimientos de los tweets a lo largo del tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasetConVaderV2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date_formatted'] = df['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_counts = df.groupby(['date', 'sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot the line plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(daily_counts.index, daily_counts[1], label='Positive Sentiment', color='green')\n",
    "ax.plot(daily_counts.index, daily_counts[0], label='Negative Sentiment', color='red')\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Count of Positive and Negative Tweets Over Time')\n",
    "ax.legend()\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b49be",
   "metadata": {},
   "source": [
    "1-02-2023 ----> 100 millones de usuarios\n",
    "https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/\n",
    "\n",
    "https://www.elmundo.es/tecnologia/2023/02/06/63e16e55fc6c83815e8b45bb.html\n",
    "https://www.semana.com/tecnologia/articulo/chatgpt-no-aguanto-al-voltaje-y-experimenta-nueva-caida-a-nivel-mundial/202325/\n",
    "https://www.reuters.com/technology/chatgpts-popularity-explodes-us-lawmakers-take-an-interest-2023-02-13/\n",
    "\n",
    "\n",
    "15-02-2023 ----> Elon Musk negative claims of chatgpt\n",
    "https://www.cnbc.com/2023/02/15/elon-musk-co-founder-of-chatgpt-creator-openai-warns-of-ai-society-risk.html\n",
    "\n",
    "Segunda semana de Marzo ----> lanzamiento de GPT-4\n",
    "\n",
    "Finales de Marzo\n",
    "\n",
    "https://efe.com/ciencia-y-tecnologia/2023-03-30/denuncian-en-estados-unidos-el-chatgpt-de-openai-y-piden-que-sea-suspendido-tras-recientes-reservas-sobre-ia/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
